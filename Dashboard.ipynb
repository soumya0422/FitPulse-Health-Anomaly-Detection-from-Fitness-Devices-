{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbkMJcTrSIoUKSrT+lem2n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumya0422/FitPulse-Health-Anomaly-Detection-from-Fitness-Devices-/blob/main/Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn streamlit pyngrok pandas requests prophet tsfresh scikit-learn matplotlib plotly --quiet"
      ],
      "metadata": {
        "id": "IxQ6NfT7HLkT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"38DbZKBCEmeKeEQCvidwnGfJErz_UDgnfdabdR8xmC2R4Hh1\")"
      ],
      "metadata": {
        "id": "QNYTJZLfHOiq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backend.py\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import traceback\n",
        "from prophet import Prophet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "app = FastAPI(title=\"FitPulse â€“ Health Anomaly Backend\")\n",
        "\n",
        "# ---------------- GLOBAL STATE ----------------\n",
        "CLEAN_DF = None\n",
        "FEATURE_DF = None\n",
        "ALERTS_DF = None\n",
        "\n",
        "# ---------------- ERROR HANDLER ----------------\n",
        "@app.exception_handler(Exception)\n",
        "async def global_exception_handler(request, exc):\n",
        "    trace = traceback.format_exc()\n",
        "    print(trace)\n",
        "    return JSONResponse(\n",
        "        status_code=500,\n",
        "        content={\"error\": \"Backend crashed\", \"trace\": trace}\n",
        "    )\n",
        "\n",
        "# ---------------- MODULE 1 â€“ PREPROCESSING ----------------\n",
        "@app.post(\"/preprocess\")\n",
        "def preprocess(file: UploadFile = File(...)):\n",
        "    global CLEAN_DF\n",
        "    df = pd.read_csv(file.file)\n",
        "    REQUIRED = [\"Id\", \"date\", \"avg_heart_rate\", \"daily_steps\", \"hours_sleep\"]\n",
        "    missing = [c for c in REQUIRED if c not in df.columns]\n",
        "    if missing:\n",
        "        return JSONResponse(status_code=400, content={\"error\": f\"Missing {missing}\"})\n",
        "    df = df.rename(columns={\n",
        "        \"Id\": \"user_id\",\n",
        "        \"avg_heart_rate\": \"heart_rate\",\n",
        "        \"daily_steps\": \"steps\",\n",
        "        \"hours_sleep\": \"sleep\"\n",
        "    })\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"date\"])\n",
        "    for col in [\"heart_rate\", \"steps\", \"sleep\"]:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    df[\"heart_rate\"].fillna(df[\"heart_rate\"].median(), inplace=True)\n",
        "    df[\"steps\"].fillna(0, inplace=True)\n",
        "    df[\"sleep\"].fillna(df[\"sleep\"].median(), inplace=True)\n",
        "    df = (\n",
        "        df.set_index(\"date\")\n",
        "          .groupby(\"user_id\")[[\"heart_rate\", \"steps\", \"sleep\"]]\n",
        "          .resample(\"D\")\n",
        "          .mean()\n",
        "          .reset_index()\n",
        "    )\n",
        "    CLEAN_DF = df\n",
        "    df.to_csv(\"clean_data.csv\", index=False)\n",
        "    return {\"status\": \"success\", \"rows\": len(df)}\n",
        "\n",
        "# ---------------- MODULE 2 â€“ FEATURE EXTRACTION + MODELING ----------------\n",
        "@app.post(\"/module2\")\n",
        "def module2():\n",
        "    global CLEAN_DF, FEATURE_DF\n",
        "    if CLEAN_DF is None:\n",
        "        return JSONResponse(status_code=400, content={\"error\": \"Run Module 1 first\"})\n",
        "    df = CLEAN_DF.sort_values([\"user_id\", \"date\"])\n",
        "    df[\"hr_7d_mean\"] = df.groupby(\"user_id\")[\"heart_rate\"].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
        "    df[\"hr_7d_std\"] = df.groupby(\"user_id\")[\"heart_rate\"].transform(lambda x: x.rolling(7, min_periods=1).std().fillna(0))\n",
        "    df[\"steps_7d_mean\"] = df.groupby(\"user_id\")[\"steps\"].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
        "    FEATURE_DF = df.copy()\n",
        "    # Prophet forecasting\n",
        "    plot = []\n",
        "    try:\n",
        "        p_df = df.groupby(\"date\")[\"heart_rate\"].mean().reset_index().rename(columns={\"date\":\"ds\",\"heart_rate\":\"y\"})\n",
        "        if len(p_df) > 10:\n",
        "            m = Prophet()\n",
        "            m.fit(p_df)\n",
        "            f = m.predict(p_df)\n",
        "            p_df[\"yhat\"] = f[\"yhat\"]\n",
        "            plot = p_df.tail(30).to_dict(\"records\")\n",
        "    except:\n",
        "        pass\n",
        "    X = FEATURE_DF[[\"heart_rate\",\"steps\",\"sleep\",\"hr_7d_mean\"]].fillna(0)\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    db = DBSCAN(eps=1.2, min_samples=5)\n",
        "    FEATURE_DF[\"cluster\"] = db.fit_predict(X)\n",
        "    FEATURE_DF[\"cluster_anomaly\"] = FEATURE_DF[\"cluster\"] == -1\n",
        "    FEATURE_DF.to_csv(\"feature_data.csv\", index=False)\n",
        "    return {\"status\": \"success\", \"sample_plot\": plot}\n",
        "\n",
        "# ---------------- MODULE 3 â€“ ANOMALY DETECTION ----------------\n",
        "@app.post(\"/module3\")\n",
        "def module3():\n",
        "    global FEATURE_DF, ALERTS_DF\n",
        "    if FEATURE_DF is None:\n",
        "        return JSONResponse(status_code=400, content={\"error\": \"Run Module 2 first\"})\n",
        "    rows = []\n",
        "    for _, r in FEATURE_DF.iterrows():\n",
        "        if r[\"heart_rate\"] > 120:\n",
        "            rows.append((r[\"user_id\"], r[\"date\"], \"heart_rate_high\"))\n",
        "        if r[\"heart_rate\"] < 40:\n",
        "            rows.append((r[\"user_id\"], r[\"date\"], \"heart_rate_low\"))\n",
        "        if r[\"sleep\"] < 4 or r[\"sleep\"] > 12:\n",
        "            rows.append((r[\"user_id\"], r[\"date\"], \"sleep_abnormal\"))\n",
        "        if r[\"cluster_anomaly\"]:\n",
        "            rows.append((r[\"user_id\"], r[\"date\"], \"cluster_outlier\"))\n",
        "    df = pd.DataFrame(rows, columns=[\"user_id\",\"date\",\"metric\"])\n",
        "    alerts = df.groupby([\"user_id\",\"metric\"]).agg(count=(\"date\",\"count\")).reset_index()\n",
        "    alerts[\"severity\"] = alerts[\"count\"].apply(lambda x: \"High\" if x>=5 else \"Medium\" if x>=3 else \"Low\")\n",
        "    ALERTS_DF = alerts\n",
        "    alerts.to_csv(\"module3_alerts.csv\", index=False)\n",
        "    return {\"status\":\"success\",\"alerts\":alerts.to_dict(\"records\")}\n",
        "\n",
        "# ---------------- DASHBOARD DATA ----------------\n",
        "@app.get(\"/dashboard\")\n",
        "def dashboard():\n",
        "    global CLEAN_DF\n",
        "    if CLEAN_DF is None:\n",
        "        return JSONResponse(status_code=400, content={\"error\":\"Run /preprocess first\"})\n",
        "    heart_rate_df = CLEAN_DF.groupby(\"date\")[\"heart_rate\"].mean().reset_index().rename(columns={\"date\":\"timestamp\"})\n",
        "    steps_df = CLEAN_DF.groupby(\"date\")[\"steps\"].mean().reset_index().rename(columns={\"date\":\"timestamp\",\"steps\":\"step_count\"})\n",
        "    summary = {\n",
        "        \"avg_heart_rate\": round(CLEAN_DF[\"heart_rate\"].mean(),2),\n",
        "        \"avg_steps\": int(CLEAN_DF[\"steps\"].mean()),\n",
        "        \"total_records\": len(CLEAN_DF)\n",
        "    }\n",
        "    return {\n",
        "        \"status\":\"success\",\n",
        "        \"heart_rate_series\": heart_rate_df.to_dict(\"records\"),\n",
        "        \"steps_series\": steps_df.to_dict(\"records\"),\n",
        "        \"summary\":summary\n",
        "    }\n",
        "\n",
        "# ---------------- MODULE 4 â€“ INSIGHTS ----------------\n",
        "@app.post(\"/module4\")\n",
        "def module4():\n",
        "    if ALERTS_DF is None or ALERTS_DF.empty:\n",
        "        return JSONResponse(status_code=400, content={\"error\":\"Run Module 3 first\"})\n",
        "    insights = [\n",
        "        {\"user_id\":r[\"user_id\"],\"severity\":r[\"severity\"],\n",
        "         \"insight\":f\"User {r['user_id']} has {r['severity']} risk due to {r['metric']}\"}\n",
        "        for _, r in ALERTS_DF.iterrows()\n",
        "    ]\n",
        "    return {\"status\":\"success\",\"insights\":insights}\n",
        "\n",
        "# ---------------- RUN Uvicorn + Ngrok ----------------\n",
        "import uvicorn, threading\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"FastAPI public URL:\", public_url)\n",
        "threading.Thread(target=uvicorn.run, kwargs={\"app\":\"backend:app\",\"host\":\"0.0.0.0\",\"port\":8000}).start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQC7lqBIHXwu",
        "outputId": "1060daa9-1e89-4760-df54-d65514a3d70b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting backend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "\n",
        "BACKEND_URL = \"http://localhost:8000\"  # Will work with ngrok URL dynamically if needed\n",
        "\n",
        "st.set_page_config(page_title=\"FitPulse â€“ Health Anomaly Dashboard\", layout=\"wide\")\n",
        "\n",
        "# ---------------- Helper ----------------\n",
        "def handle_response(res, label=\"\"):\n",
        "    st.write(f\"ðŸ” {label} status:\", res.status_code)\n",
        "    content_type = res.headers.get(\"content-type\",\"\")\n",
        "    if content_type.startswith(\"application/json\"):\n",
        "        return res.json()\n",
        "    st.error(\"âŒ Backend returned NON-JSON response\")\n",
        "    st.code(res.text)\n",
        "    return None\n",
        "\n",
        "# ---------------- Sidebar ----------------\n",
        "st.sidebar.header(\"ðŸ“‚ Upload Fitness Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload CSV/JSON\", type=[\"csv\",\"json\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    with st.spinner(\"Preprocessing data...\"):\n",
        "        try:\n",
        "            res = requests.post(f\"{BACKEND_URL}/preprocess\", files={\"file\":uploaded_file})\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            st.sidebar.error(f\"Connection failed: {e}\")\n",
        "            st.stop()\n",
        "        data = handle_response(res,\"Preprocess\")\n",
        "        if not data: st.stop()\n",
        "        if res.status_code == 200:\n",
        "            st.sidebar.success(\"âœ… Data processed\")\n",
        "            st.sidebar.metric(\"Rows Loaded\", data.get(\"rows\"))\n",
        "        else:\n",
        "            st.sidebar.error(data.get(\"error\"))\n",
        "            st.stop()\n",
        "\n",
        "# ---------------- Tabs ----------------\n",
        "tab1, tab2, tab3, tab4 = st.tabs([\"ðŸ“Š Overview\",\"ðŸ“ˆ Health Dashboard\",\"ðŸš¨ Anomaly Detection\",\"ðŸ§  Insights & Reports\"])\n",
        "\n",
        "# ===== TAB1: Overview =====\n",
        "with tab1:\n",
        "    st.subheader(\"Dataset Overview\")\n",
        "    try:\n",
        "        df = pd.read_csv(\"clean_data.csv\")\n",
        "        st.dataframe(df.head(50), use_container_width=True)\n",
        "        c1,c2,c3 = st.columns(3)\n",
        "        c1.metric(\"Users\", df[\"user_id\"].nunique())\n",
        "        c2.metric(\"Days\", df[\"date\"].nunique())\n",
        "        c3.metric(\"Avg HR\", round(df[\"heart_rate\"].mean(),1))\n",
        "    except FileNotFoundError:\n",
        "        st.info(\"â¬…ï¸ Upload data to see overview\")\n",
        "        st.stop()\n",
        "\n",
        "# ===== TAB2: Health Dashboard =====\n",
        "with tab2:\n",
        "    st.subheader(\"ðŸ“Š Health Dashboard\")\n",
        "    if st.button(\"Load Dashboard\"):\n",
        "        with st.spinner(\"Fetching dashboard data...\"):\n",
        "            try:\n",
        "                res = requests.get(f\"{BACKEND_URL}/dashboard\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                st.error(f\"Connection error: {e}\")\n",
        "                st.stop()\n",
        "            data = handle_response(res,\"Dashboard\")\n",
        "            if not data: st.stop()\n",
        "            hr_df = pd.DataFrame(data[\"heart_rate_series\"])\n",
        "            steps_df = pd.DataFrame(data[\"steps_series\"])\n",
        "            if not hr_df.empty:\n",
        "                fig = px.line(hr_df, x=\"timestamp\", y=\"heart_rate\", title=\"â¤ï¸ Heart Rate Over Time\")\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "            if not steps_df.empty:\n",
        "                fig = px.bar(steps_df, x=\"timestamp\", y=\"step_count\", title=\"ðŸš¶ Steps Over Time\")\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# ===== TAB3: Anomaly Detection =====\n",
        "with tab3:\n",
        "    st.subheader(\"ðŸš¨ Anomaly Detection\")\n",
        "    if st.button(\"Detect Anomalies\"):\n",
        "        with st.spinner(\"Scanning health metrics...\"):\n",
        "\n",
        "            # 1ï¸âƒ£ Run Module 2 first (feature extraction + clustering)\n",
        "            try:\n",
        "                res2 = requests.post(f\"{BACKEND_URL}/module2\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                st.error(f\"Connection failed (Module 2): {e}\")\n",
        "                st.stop()\n",
        "\n",
        "            data2 = handle_response(res2, \"Module 2\")\n",
        "            if not data2: st.stop()\n",
        "            if res2.status_code != 200:\n",
        "                st.error(\"Module 2 failed: \" + data2.get(\"error\", \"Unknown\"))\n",
        "                st.stop()\n",
        "\n",
        "            # 2ï¸âƒ£ Now run Module 3 (anomaly detection)\n",
        "            try:\n",
        "                res3 = requests.post(f\"{BACKEND_URL}/module3\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                st.error(f\"Connection failed (Module 3): {e}\")\n",
        "                st.stop()\n",
        "\n",
        "            data3 = handle_response(res3, \"Module 3\")\n",
        "            if not data3: st.stop()\n",
        "\n",
        "            if res3.status_code == 200:\n",
        "                alerts = pd.DataFrame(data3[\"alerts\"])\n",
        "                if alerts.empty:\n",
        "                    st.success(\"No anomalies detected ðŸŽ‰\")\n",
        "                else:\n",
        "                    st.dataframe(alerts, use_container_width=True)\n",
        "                    fig = px.bar(\n",
        "                        alerts,\n",
        "                        x=\"metric\",\n",
        "                        y=\"count\",\n",
        "                        color=\"severity\",\n",
        "                        title=\"Detected Health Alerts\"\n",
        "                    )\n",
        "                    st.plotly_chart(fig, use_container_width=True)\n",
        "            else:\n",
        "                st.error(data3.get(\"error\"))\n",
        "\n",
        "\n",
        "# ===== TAB4: Insights & Reports =====\n",
        "with tab4:\n",
        "    st.subheader(\"ðŸ§  Health Risk Insights\")\n",
        "    if st.button(\"Generate Insights\"):\n",
        "        with st.spinner(\"Generating insights...\"):\n",
        "            try:\n",
        "                res = requests.post(f\"{BACKEND_URL}/module4\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                st.error(f\"Connection failed: {e}\")\n",
        "                st.stop()\n",
        "            data = handle_response(res,\"Module 4\")\n",
        "            if not data: st.stop()\n",
        "            if res.status_code == 200:\n",
        "                for i in data[\"insights\"]:\n",
        "                    st.warning(f\"User {i['user_id']} â€” {i['severity']} Risk\\n{i['insight']}\")\n",
        "            else:\n",
        "                st.error(data.get(\"error\"))\n",
        "    st.divider()\n",
        "    st.subheader(\"â¬‡ï¸ Download Reports\")\n",
        "    try:\n",
        "        st.download_button(\"Download Clean Data\", open(\"clean_data.csv\",\"rb\"), file_name=\"clean_data.csv\")\n",
        "        st.download_button(\"Download Anomaly Report\", open(\"module3_alerts.csv\",\"rb\"), file_name=\"health_alerts.csv\")\n",
        "    except FileNotFoundError:\n",
        "        st.info(\"Run analysis to generate reports\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGyKBEA0HssT",
        "outputId": "bc607bc2-ea6d-4103-d70f-62d05a042746"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Launch Streamlit via Ngrok\n",
        "# ===============================\n",
        "import threading, time\n",
        "def run_streamlit():\n",
        "    !streamlit run app.py --server.port 8501 --server.address 0.0.0.0\n",
        "threading.Thread(target=run_streamlit).start()\n",
        "time.sleep(5)\n",
        "streamlit_url = ngrok.connect(8501)\n",
        "print(\"Streamlit public URL:\", streamlit_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAV7z67vHwz0",
        "outputId": "8488e2e7-85ef-4bb2-947c-6ec30cd9a359"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "2026-01-13 20:49:44.614 Port 8501 is already in use\n",
            "Streamlit public URL: NgrokTunnel: \"https://renowned-zuri-coleopterous.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}